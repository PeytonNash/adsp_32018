{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9359258d",
   "metadata": {},
   "source": [
    "# ADSP 32018: Final Project\n",
    "## Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "Peyton Nash\n",
    "\n",
    "### Project Description\n",
    "In March of 2023, Goldman Sachs published a report, indicating that ~25% of the tasks in US and Europe can be automated using AI.  However, not all industries will be affected equally. According to the report, certain jobs, like office tasks, legal, architecture, and social sciences have a potential for 30%+ automation, while positions like construction, installation, and building maintenance are going to be largely unaffected.\n",
    "\n",
    "In July of 2025, Microsoft published an in-depth studyLinks to an external site. based on 200,000 anonymized conversations with Microsoft Copilot, aiming to understand how generative AI is actually being used in the workplace and which professions are being most affected.\n",
    "\n",
    "The researchers separated what users intended to do from what the AI actually delivered. They then mapped both to detailed job functions defined by O*NET. Using this framework, along with indicators of task success and coverage, they developed an “AI applicability score” for every occupation.\n",
    "\n",
    "The findings are clear. Generative AI excels at tasks like information gathering, writing, and communication. It is already transforming knowledge and service-based roles. However, it has limited usefulness in jobs that rely on physical effort.\n",
    "One of the most surprising insights? There’s little connection between AI’s impact and factors like income or education level. This challenges long-held assumptions about which roles are most at risk of disruption.\n",
    "\n",
    "You can also find supporting evidence in the Facebook Research paper, which highlights Moravec’s Paradox. This thesis posits that the hardest problems in AI involve sensorimotor skills rather than abstract thought or reasoning. Notably, these findings coincide with predictions made by Goldman Sachs.\n",
    "\n",
    "For this final project, I have prepared a collection of ~200K news articles on our favorite topics, data science, machine learning, and artificial intelligence. Your task is to identify what industries are going to be most impacted by AI over the next several years, based on the information/insights you can extract from this text corpus.\n",
    "\n",
    "Your goal is to provide actionable recommendations on what can be done with AI to automate the jobs, improve employee productivity, and generally make AI adoption successful. Please pay attention to the introduction of novel technologies and algorithms, such as AI for image generation and Conversational AI, as they represent the entire paradigm shift in adoption of AI technologies and data science in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f44736",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecc112f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re, math, gc, itertools, warnings, os, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import tldextract\n",
    "from nltk import download\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2634e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed54e388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 200760\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "\n",
    "# Check dimensions\n",
    "print(f'Number of articles: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d8aa5",
   "metadata": {},
   "source": [
    "### Initial Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fba0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://businessnewsthisweek.com/business/infog...</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Infogain AI Business Solutions Now Available i...</td>\n",
       "      <td>\\n\\nInfogain AI Business Solutions Now Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.huewire.com/how-you-should-validate...</td>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>en</td>\n",
       "      <td>How You Should Validate Machine Learning Model...</td>\n",
       "      <td>\\n\\nHow You Should Validate Machine Learning M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.huewire.com/vise-intelligence-is-a-...</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>en</td>\n",
       "      <td>Vise Intelligence is a new AI to assist — not ...</td>\n",
       "      <td>\\n\\nVise Intelligence is a new AI to assist — ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://abcnews.go.com/Technology/google-makes...</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>en</td>\n",
       "      <td>Google makes adjustments to AI Overviews after...</td>\n",
       "      <td>\\n\\nGoogle makes adjustments to AI Overviews a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://betanews.com/2023/06/08/wordpress-ai-a...</td>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>en</td>\n",
       "      <td>WordPress' AI Assistant can write blog posts...</td>\n",
       "      <td>\\n\\n\\n  WordPress' AI Assistant can write blog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language  \\\n",
       "0  http://businessnewsthisweek.com/business/infog...  2023-05-20       en   \n",
       "1  http://www.huewire.com/how-you-should-validate...  2023-07-21       en   \n",
       "2  http://www.huewire.com/vise-intelligence-is-a-...  2023-09-29       en   \n",
       "3  https://abcnews.go.com/Technology/google-makes...  2024-06-03       en   \n",
       "4  https://betanews.com/2023/06/08/wordpress-ai-a...  2023-06-08       en   \n",
       "\n",
       "                                               title  \\\n",
       "0  Infogain AI Business Solutions Now Available i...   \n",
       "1  How You Should Validate Machine Learning Model...   \n",
       "2  Vise Intelligence is a new AI to assist — not ...   \n",
       "3  Google makes adjustments to AI Overviews after...   \n",
       "4    WordPress' AI Assistant can write blog posts...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nInfogain AI Business Solutions Now Availab...  \n",
       "1  \\n\\nHow You Should Validate Machine Learning M...  \n",
       "2  \\n\\nVise Intelligence is a new AI to assist — ...  \n",
       "3  \\n\\nGoogle makes adjustments to AI Overviews a...  \n",
       "4  \\n\\n\\n  WordPress' AI Assistant can write blog...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc0228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "en    200760\n",
      "Name: language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check language of articles\n",
    "print(df.groupby('language')['language'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f505eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest scrape data: 2022-01-01\n",
      "Latest scrape data: 2025-07-22\n"
     ]
    }
   ],
   "source": [
    "# Check scrape data of articles\n",
    "print(f'Earliest scrape data: {df[\"date\"].min()}')\n",
    "print(f'Latest scrape data: {df[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16fd885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique domains: 5119\n",
      "\n",
      "Most common sources:\n",
      " domain\n",
      "rawpixel        8831\n",
      "citylife        4040\n",
      "menafn          3847\n",
      "einpresswire    3637\n",
      "indiatimes      3594\n",
      "prnewswire      3393\n",
      "nasdaq          2473\n",
      "yahoo           1982\n",
      "levels          1576\n",
      "livemint        1479\n",
      "Name: domain, dtype: int64\n",
      "\n",
      "Number of sources with more than 100 articles: 394\n",
      "\n",
      "Percentage of articles published by sources with more than 100 articles: 70.60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the number of sources\n",
    "df['domain'] = df['url'].apply(lambda x: tldextract.extract(x).domain)\n",
    "print(f'Number of unique domains: {df[\"domain\"].unique().shape[0]}\\n')\n",
    "\n",
    "# Check most common sources\n",
    "source_count = df.groupby('domain')['domain'].count().sort_values(ascending=False)\n",
    "print(f'Most common sources:\\n {source_count[:10]}\\n')\n",
    "print(f'Number of sources with more than 100 articles: {source_count[source_count>100].shape[0]}\\n')\n",
    "print(f'Percentage of articles published by sources with more than 100 articles: {100 * source_count[source_count>100].sum()/source_count.sum():.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299e589",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d99f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove large irrelevant domains\n",
    "domain_drop = ['rawpixel', 'levels', 'mexc']\n",
    "df = df[~df['domain'].isin(domain_drop)]\n",
    "\n",
    "print(f'Number of remaining articles: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d30e55",
   "metadata": {},
   "source": [
    "#### Remove Webscrape Remnants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a56d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/peytonnash/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Identify domains so that articles are not grouped based on domain name\n",
    "download('words')\n",
    "\n",
    "words_eng = set(words.words())\n",
    "domains = list(df['domain'].sort_values().unique())\n",
    "domains_clean = []\n",
    "\n",
    "for item in domains:\n",
    "    if item.lower() not in words_eng:\n",
    "        domains_clean.append(item)\n",
    "\n",
    "del words, domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "842522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of common boilerplate text\n",
    "boilerplate_patterns = [\n",
    "    r\" subscribe[.\\s]| newsletter[.\\s]| cookie policy[.\\s]| privacy policy[.\\s]| terms of service[.\\s]\",\n",
    "    r\" all rights reserved| copyright\\s{1,3}[0-9]{4}[.\\s]\",\n",
    "    r\" advertisement[.\\s]| sponsored[.\\s]| sponsored content[.\\s]| promoted[.\\s]| story continues below[.\\s]\",\n",
    "    r\" most read[.\\s]| most popular[.\\s]| trending[.\\s]| read more[.\\s]| you might also like[.\\s]| sitemap[.\\s]| breaking news[.\\s]\",\n",
    "    r\" share this[.\\s]| follow us[.\\s]| contact us[.\\s]| sign up[.\\s]| log in[.\\s]| register[.\\s]| share on facebook[.\\s]\",\n",
    "]\n",
    "\n",
    "# r'subscribe | newsletter | cookie policy | privacy policy | terms of service | all rights reserved | copyright\\s{1,3}[0-9]{4} | advertisement |sponsored | sponsored content |promoted | story continues below | most read | most popular | trending | read more | you might also like | sitemap | share this | follow us | contact us | sign up | log in | register |\\n|\\d{1,2}\\:\\d{1,2}|[.:-]| / '\n",
    "\n",
    "# Combine the boiler plate text to split on it below\n",
    "split_pattern = '|'.join(boilerplate_patterns) + '|\\n|\\d{1,2}\\:\\d{1,2}|\\d{2}[-/]\\d{2}[-/]\\d{4}| / '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fba627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean up the web-scrape remnants\n",
    "def clean_text(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Remove urls/emails\n",
    "    text = re.sub(r\"http[s]?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+\", \" \", text)\n",
    "\n",
    "    # Remove non-English characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Normalize bullets/long dashes\n",
    "    text = text.replace(\"•\", \" \").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "\n",
    "    # Split the text into lines for line-level filtering\n",
    "    lines = [line.strip() for line in re.split(split_pattern, text, flags=re.IGNORECASE) if line]\n",
    "\n",
    "    pruned = []\n",
    "    for ln in lines:\n",
    "        if not ln:\n",
    "            continue\n",
    "        # Discard short lines with a high proportion of non-alphabetical characters\n",
    "        if len(ln) < 30 and sum(c.isalpha() for c in ln) < 20:\n",
    "            continue\n",
    "        # Strip all-caps headlines unless reasonably long\n",
    "        if ln.isupper() and len(ln) < 80:\n",
    "            continue\n",
    "        # Identify lines with more than 40% of the characters are non-alphabetical\n",
    "        alpha = sum(c.isalpha() for c in ln)\n",
    "        if alpha == 0 or alpha / max(len(ln), 1) < 0.4:\n",
    "            continue\n",
    "        if len(sent_tokenize(ln, language='english')) < 2:\n",
    "            continue\n",
    "        pruned.append(ln)\n",
    "\n",
    "    # Combine the lines into a single text\n",
    "    text = \" \".join(pruned)\n",
    "\n",
    "    # Remove most punctuation except intra-word hyphens/apostrophes\n",
    "    text = re.sub(r\"[^\\w\\s'\\-$%.,:;/]\", ' ', text)\n",
    "\n",
    "    # Remove common webscrape remnants\n",
    "    text = re.sub(split_pattern, ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove domain names\n",
    "    domain_pattern = r\"(?:\\s(?:{})(?=\\s))\".format(\"|\".join(domains_clean))\n",
    "    text = re.sub(domain_pattern, ' ', text, re.IGNORECASE)\n",
    "\n",
    "    # Reduce repeated whitespace to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Remove text in brackets\n",
    "    text = re.sub(r\"\\[[^\\]]{0,60}\\]\", \" \", text)\n",
    "    text = re.sub(r\"\\([^)<>]{0,60}\\)\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def is_english(text: str) -> bool:\n",
    "    try:\n",
    "        if len(text) < 200:\n",
    "            return True\n",
    "        return detect(text) == \"en\"\n",
    "    except Exception:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f80c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf47827686ff4fafa68b60f77b804b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=18251), Label(value='0 / 18251')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify body text\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "df['text_clean'] = df['text'].parallel_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb42ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c47138e860479b9f0657fbf1536242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=18251), Label(value='0 / 18251')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify English text\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "df['english'] = df['text_clean'].parallel_apply(is_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e3cb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 200760\n",
      "Number of documents with non-empty text: 197620\n",
      "Number of English documents: 200575\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of documents: {len(df)}')\n",
    "print(f'Number of documents with non-empty text: {len(df[df[\"text_clean\"] != \"\"])}')\n",
    "print(f'Number of English documents: {len(df[df[\"english\"]==True])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f512ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('output_data/df_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40867e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83baa2835804bd1a3f8773cdadb7ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=18251), Label(value='0 / 18251')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length range: 0 to 236068\n",
      "Number of articles after removing long and short articles: 199196\n",
      "Length range after removing long and short articles: 59 to 179\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the cleaned text\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "df['text_length'] = df['text_clean'].parallel_apply(lambda x: len(x))\n",
    "\n",
    "print(f'Length range: {df[\"text_length\"].min()} to {df[\"text_length\"].max()}')\n",
    "\n",
    "# Remove the top and bottom five percent longest and shortest texts\n",
    "pctl = np.percentile(df[df['text_length'] > 0]['text_length'], [.15, .95])\n",
    "df = df[(df['text_length']<=pctl[0]) | (df['text_length']>=pctl[1])]\n",
    "\n",
    "print(f'Number of articles after removing long and short articles: {len(df)}')\n",
    "print(f'Length range after removing long and short articles: {int(np.floor(pctl[0]))} to {int(np.ceil(pctl[1]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f8e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after removing non-English: 199011\n"
     ]
    }
   ],
   "source": [
    "# Remove non-English documents\n",
    "df = df[df['english'] == True]\n",
    "print(f'Number of articles after removing non-English: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f532c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "df = df.drop(['url', 'english', 'language', 'text', 'domain', 'text_length'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8c0af",
   "metadata": {},
   "source": [
    "#### Detect Irrelevant Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7353d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of keywords to detect irrelevant articles\n",
    "relevance_keywords_ai = [\n",
    "    # General AI terms\n",
    "    \"artificial intelligence\",\n",
    "    \"ai\",\n",
    "    \"machine learning\",\n",
    "    \"ml\",\n",
    "    \"deep learning\",\n",
    "    \"neural networks\",\n",
    "    \"large language models\",\n",
    "    \"llm\",\n",
    "    \"generative\",\n",
    "    \"gen \",\n",
    "    \"chatbot\",\n",
    "    \"natural language processing\",\n",
    "    \"nlp\",\n",
    "    \"computer vision\",\n",
    "    \"predictive analytics\",\n",
    "    \"automation technology\",\n",
    "    \"intelligent systems\",\n",
    "    \"cognitive computing\",\n",
    "\n",
    "    # Workplace & adoption contexts\n",
    "    \"algorithm\",\n",
    "    \"algorithmic\",\n",
    "\n",
    "    # Tools & platforms\n",
    "    \"chatgpt\",\n",
    "    \"openai\",\n",
    "    \"bard\",\n",
    "    \"claude\",\n",
    "    \"copilot\",\n",
    "    \"grok\",\n",
    "    \"anthropic\",\n",
    "    \"xai\",\n",
    "    \"autonomous\",\n",
    "    \"self-driving\",\n",
    "\n",
    "    # Implementation areas\n",
    "    \"process automation\",\n",
    "    \"business automation\",\n",
    "    \"workflow automation\",\n",
    "    \"robotic\",\n",
    "    \"data-driven algorithms\",\n",
    "    \"predictive modeling\",\n",
    "]\n",
    "\n",
    "relevance_keywords_prod = [\n",
    "    # Productivity & efficiency\n",
    "    \"productivity\",\n",
    "    \"efficiency\",\n",
    "    \"output\",\n",
    "    \"performance\",\n",
    "    \"optimization\",\n",
    "    \"throughput\",\n",
    "    \"automation\",\n",
    "    \"process\",\n",
    "    \"streamlining\",\n",
    "    \"scalability\",\n",
    "    \"time savings\",\n",
    "    \"savings\",\n",
    "    \"reduction\",\n",
    "    \"cost\",\n",
    "    \"automatic\",\n",
    "    \"automation\",\n",
    "    \"robot\",\n",
    " \n",
    "    # Economic impact\n",
    "    \"economy\",\n",
    "    \"economic\",\n",
    "    \"profitability\",\n",
    "    \"competitiveness\",\n",
    "    \"performance\",\n",
    "    \"margins\",\n",
    "    \"return on investment\",\n",
    "    \"roi\",\n",
    "    \"production\",\n",
    "    \"scaling\",\n",
    "    \"scale\",\n",
    "\n",
    "    # Workforce restructuring\n",
    "    \"work\",\n",
    "    \"workforce\",\n",
    "    \"job\",\n",
    "    \"displacement\",\n",
    "    \"reduction\",\n",
    "    \"downsizing\",\n",
    "    \"redundancy\",\n",
    "    \"layoffs\",\n",
    "    \"cuts\",\n",
    "    \"outsourcing\",\n",
    "    \"restructuring\",\n",
    "    \"labor\",\n",
    "    \"reduction\",\n",
    "\n",
    "    # Job transformation\n",
    "    \"reskilling\",\n",
    "    \"upskilling\",\n",
    "    \"redeployment\",\n",
    "    \"retraining\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9ddf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(text: str) -> np.ndarray:\n",
    "    text = text.lower()\n",
    "    patterns_ai = [re.compile(r\"\\b\" + re.escape(k) + r\"s?\\b\", re.I) for k in relevance_keywords_ai]\n",
    "    patterns_prod = [re.compile(r\"\\b\" + re.escape(k) + r\"s?\\b\", re.I) for k in relevance_keywords_prod]\n",
    "    score_ai = (sum(1 for p in patterns_ai if re.search(p, text)))\n",
    "    score_prod = (sum(1 for p in patterns_prod if re.search(p, text)))\n",
    "\n",
    "    return score_ai, score_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66cd12da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eae640af7ed438aa0bf1d1240b9a8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=18092), Label(value='0 / 18092')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify relevant articles\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "df[['score_ai', 'score_prod']] = pd.DataFrame(df['text_clean'].parallel_apply(relevance_score).tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06d8ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with no keywords from one or both categories: 57975\n",
      "Number of articles with no keywords from either categories: 8393\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of articles with no keywords from one or both categories: {len(df[(df.score_prod == 0) | (df.score_ai == 0)])}')\n",
    "print(f'Number of articles with no keywords from either categories: {len(df[(df.score_prod == 0) & (df.score_ai == 0)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae4df6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('output_data/df_relevant.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da758bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('output_data/df_relevant.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbd1efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after removing irrelevant: 190618\n"
     ]
    }
   ],
   "source": [
    "# Remove irrelevant articles\n",
    "df = df[(df.score_prod > 0) | (df.score_ai > 0)]\n",
    "print(f'Number of articles after removing irrelevant: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50005a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the minhash of the text\n",
    "def get_minhash(text: str, num_perm: int = 128):\n",
    "    # Get the minhash of the text\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for token in set(text.split()):\n",
    "        m.update(token.encode(\"utf8\"))\n",
    "    return m\n",
    "\n",
    "# Deduplication through LSH\n",
    "def dedupe_lsh(docs, threshold: float = 0.9, num_perm: int = 128):\n",
    "    # Define an LSH object\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    minhashes = []\n",
    "\n",
    "    # Get the minhashes for each article\n",
    "    for i, doc in enumerate(docs):\n",
    "        m = get_minhash(doc, num_perm=num_perm)\n",
    "        lsh.insert(f\"doc_{i}\", m)\n",
    "        minhashes.append(m)\n",
    "\n",
    "    # Create objects to store outputs\n",
    "    visited = set()\n",
    "    groups = []\n",
    "\n",
    "    # Identify articles with similar texts\n",
    "    for i, m in enumerate(minhashes):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        dup_idxs = [int(j.replace(\"doc_\", \"\")) for j in lsh.query(m)]\n",
    "        for j in dup_idxs:\n",
    "            visited.add(j)\n",
    "        groups.append(sorted(dup_idxs))\n",
    "\n",
    "    # Create ouput\n",
    "    keep_mask = [True] * len(docs)\n",
    "    for g in groups:\n",
    "        for j in g[1:]:\n",
    "            keep_mask[j] = False\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a02c3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "df['dupe'] = dedupe_lsh(df['text_clean'].tolist(), threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50d12e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles that are not duplicates: 162130\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of articles that are not duplicates: {len(df[df.dupe == True])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98bbf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the deduplicated data\n",
    "df = df[df.dupe == True]\n",
    "df.to_parquet('output_data/df_dedupe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d676240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
